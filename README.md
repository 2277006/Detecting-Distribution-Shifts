# Detecting-Distribution-Shifts

One of the most common challenges faced when deploying machine learning models in real-world environments is distribution shift. Typically, engineers train models using carefully curated datasets with high-quality, well-distributed samples to maximize accuracy. However, in actual deployment scenarios, the input data may significantly differ from the training data. In applications like image classification for e-commerce platforms, for example, user-uploaded product images may have lower quality or resolution, and the distribution of product categories may vary substantially compared to the training phase.
To simulate such real-world conditions, this study utilizes the FashionMNIST dataset to explore two major types of distribution shifts. The first is a covariate shift, implemented by degrading image quality through resolution reduction and brightness enhancement. The second is a label shift, where the frequency of a specific class (sneakers) is reduced by 90%, realistically modeling changes in user behavior. Through statistical techniques, a domain classifier, and an evaluation of a classification model (ResNet-18), this research aims to analyze the distribution shift and interpret its impact on model performance.
